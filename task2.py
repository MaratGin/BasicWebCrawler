from bs4 import BeautifulSoup
import os
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
import string
import pymorphy3
import re

tokens = []
lemmas = {}

path = "saved1"
nltk.download('punkt')
nltk.download('stopwords')
stop_words = stopwords.words('russian')
stop_words.extend(['—ç—Ç–æ', '—á—Ç√≤','–≤—Å—ë', '—Ç–µ', '–ë', 'W', '–£','0','–û','—É',
                   'sports', 'sportsru', 'ufc', '–±—É–∫–º–µ–∫–µ—Ä', '–π', "—Ñ–≥",
                   "‚óè" , ",", " ,", "—Ä", "—Ö","–±", "–ø—Ä", "–≥–Ω","—Å–∞"])
bad_formatted_words = ['–í–∞–ª—å–µ–∫–∞–Ω–æ', '–ø–ª–∞—á—É—Ç–∂–∞–ª—É—é—Ç—Å—è—Å–∫—É–ª—è—Ç–Ω—ã—Ç—å', '–ø–ª–∞—á—É—Ç–∂–∞–ª—É—é—Ç—Å—è—Å–∫—É–ª—è—Ç–Ω–æ—é—Ç', '–∫–ª—É–±–∫–æ—Ç–æ—Ä—ã–π']
stop_words = set(stopwords.words())
spec_chars = string.punctuation + ('¬´¬ª‚Äî‚Ä¶-‚Äô¬©,‚Äù‚óè‚âà„Å§ü§ìüòÅü´®‚öΩüíÉüé≠üòÇü§îü§£üèÜüëÄ'
                                   'üëë‚Äì‚ù§‚ú®üëåüëçüßê‚ñ°üéÖüö®üëèü§óüá¶üá∑‚ùì‚ÇΩ‚Ññü§©‡ººüî•üç∫ü§ù'
                                   'üòçüòÑü§Øü§î‚ùìüáßüá∑ü§î‚ùåüôÑüíäüÖ∞Ô∏èüí£üìπüèªüíîüîóüòÖ')
nltk.download('omw-1.4')
text_pages = []

def tokenization(started_text):
    # —É–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–º–∞–π–ª–∏–∫–∏ –∏ —Å–∏–º–≤–æ–ª—ã
    text = "".join([ch for ch in started_text if ch not in spec_chars])
    # —É–±–∏—Ä–∞–µ–º —á–∏—Å–ª–∞
    text = "".join([ch for ch in text if ch not in string.digits])


    # –ø–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã
    tokens = word_tokenize(text)
    # —É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    tokens = [word.strip() for word in tokens if word not in stop_words]

    tokens = [word for word in tokens if all(sub not in word for sub in bad_formatted_words)]




    filtered_tokens = []
    for token in tokens:
        # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        if re.search('[a-zA-Z]+', token) is None:
            # –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ –Ω–æ–≤—ã–π –º–∞—Å—Å–∏–≤
            if len(token) > 2:
                filtered_tokens.append(token.lower())

    filtered_tokens = [word.strip() for word in filtered_tokens if word not in stop_words]
    return filtered_tokens

def lemmatize(tokens):
    # –Ω–∞—Ö–æ–¥–∏–º –ª–µ–º–º—ã –≤ —Å–ø–∏—Å–∫–µ —Ç–æ–∫–µ–Ω–æ–≤
    morph = pymorphy3.MorphAnalyzer()
    for token in tokens:
        morphed = morph.parse(str(token))[0]
        normal = morphed.normal_form
        if normal in lemmas:
            lemmas[normal].add(token)
        else:
            if normal == token:
                lemmas[normal] = {token}
            else:
                lemmas[normal] = {normal}
                lemmas[normal].add(token)

def save_tokens():
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–∞–π–ª–µ
    with open("tokens.txt", "a") as myfile:
        for token in tokens:
            myfile.write(token + "\n")


def save_lemmas():
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–µ–º–º—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–∞–π–ª–µ
    with open("lemmas.txt", "a") as myfile:
        for lemma, tokens_list in lemmas.items():
            myfile.write(lemma +": ")
            for token in tokens_list:
                myfile.write(str(token) + ", ")
            myfile.write("\n")

if __name__ == "__main__":
    for filename in os.listdir(path):
        if filename.endswith(".html"):
            with open(path + "/" + filename) as file:
                soup = BeautifulSoup(file.read(), features="html.parser")
                page_text = " ".join(soup.stripped_strings)
                fetched_tokens = tokenization(page_text)
                tokens.extend(fetched_tokens)
            continue
        else:
            continue
    tokens = list(set(tokens))
    lemmatize(tokens)
    save_tokens()
    save_lemmas()
