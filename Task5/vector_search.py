from bs4 import BeautifulSoup
import os
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
import string
import pymorphy3
import re
import math
import task2
from os import listdir, path
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial import distance

token_path = "../tokens.txt"
lemmas_path = "../lemmas.txt"
index_path = "../index1.txt"
pages_path = "../saved1"
tf_idf_path = "../Task4/tf_idf_lemmas"


tokens = {}
lemmas = {}
path = "saved1"
nltk.download('punkt')
nltk.download('stopwords')
stop_words = stopwords.words('russian')
stop_words.extend(['—ç—Ç–æ', '—á—Ç√≤','–≤—Å—ë', '—Ç–µ', '–ë', 'W', '–£','0','–û','—É',
                   'sports', 'sportsru', 'ufc', '–±—É–∫–º–µ–∫–µ—Ä', '–π', "—Ñ–≥",
                   "‚óè" , ",", " ,", "—Ä", "—Ö","–±", "–ø—Ä", "–≥–Ω","—Å–∞"])
bad_formatted_words = ['–í–∞–ª—å–µ–∫–∞–Ω–æ', '–ø–ª–∞—á—É—Ç–∂–∞–ª—É—é—Ç—Å—è—Å–∫—É–ª—è—Ç–Ω—ã—Ç—å', '–ø–ª–∞—á—É—Ç–∂–∞–ª—É—é—Ç—Å—è—Å–∫—É–ª—è—Ç–Ω–æ—é—Ç', '–∫–ª—É–±–∫–æ—Ç–æ—Ä—ã–π']
stop_words = set(stopwords.words())
spec_chars = string.punctuation + ('¬´¬ª‚Äî‚Ä¶-‚Äô¬©,‚Äù‚óè‚âà„Å§ü§ìüòÅü´®‚öΩüíÉüé≠üòÇü§îü§£üèÜüëÄ'
                                   'üëë‚Äì‚ù§‚ú®üëåüëçüßê‚ñ°üéÖüö®üëèü§óüá¶üá∑‚ùì‚ÇΩ‚Ññü§©‡ººüî•üç∫ü§ù'
                                   'üòçüòÑü§Øü§î‚ùìüáßüá∑ü§î‚ùåüôÑüíäüÖ∞Ô∏èüí£üìπüèªüíîüîóüòÖ')
nltk.download('omw-1.4')
page_texts = [None] * 102

morph = pymorphy3.MorphAnalyzer()


def tokenization(started_text):
    # —É–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–º–∞–π–ª–∏–∫–∏ –∏ —Å–∏–º–≤–æ–ª—ã
    text = "".join([ch for ch in started_text if ch not in spec_chars])
    # —É–±–∏—Ä–∞–µ–º —á–∏—Å–ª–∞
    text = "".join([ch for ch in text if ch not in string.digits])
    # –ø–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã
    tokens = word_tokenize(text)
    # —É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    tokens = [word.strip() for word in tokens if word not in stop_words]

    tokens = [word for word in tokens if all(sub not in word for sub in bad_formatted_words)]

    filtered_tokens = []
    for token in tokens:
        # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        if re.search('[a-zA-Z]+', token) is None:
            # –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ –Ω–æ–≤—ã–π –º–∞—Å—Å–∏–≤
            if len(token) > 2:
                filtered_tokens.append(token.lower())

    filtered_tokens = [word.strip() for word in filtered_tokens if word not in stop_words]
    return filtered_tokens

def lemmatize(tokens):
    # –Ω–∞—Ö–æ–¥–∏–º –ª–µ–º–º—ã –≤ —Å–ø–∏—Å–∫–µ —Ç–æ–∫–µ–Ω–æ–≤
    result_lemmas = []
    morph = pymorphy3.MorphAnalyzer()
    for token in tokens:
        morphed = morph.parse(str(token))[0]
        normal = morphed.normal_form
        if normal not in lemmas:
            result_lemmas.append(normal)
    return result_lemmas

def get_tokens():
    tokens = []
    with open(token_path, 'r') as file:
        tokens = [token.strip() for token in file.readlines()]
        return tokens

def get_links():
    with open(index_path, 'r') as file:
        lines = file.readlines()
        links = {}
        for line in lines:
            key, value = line.split('-----')
            value = value.strip()
            key = re.sub(r"\D", "", key)
            links[key] = value
        return links


def vectorize(value, lemmas):
    vector = np.zeros(len(lemmas))
    tokens = tokenization(value)
    for token in tokens:
        morphed = morph.parse(token)[0]
        lemma = morphed.normal_form if morphed.normalized.is_known else token.lower()
        if lemma in lemmas:
            vector[lemmas.index(lemma)] = 1
    return vector

def get_lemmas():
    tokens = []
    with open(lemmas_path, 'r') as file:
        lines = file.readlines()
        lemmas = []
        for line in lines:
            key, value = line.split(': ')
            sub_tokens = value.split(', ')
            lemmas.append(key)

        return lemmas

def get_tf_idf(lemmas_list):
    file_names = listdir(tf_idf_path)
    zero_matrix = np.zeros((len(file_names), len(lemmas_list)))
    for file in file_names:
        if "page" in file:
            file_number = int(re.search('\\d+', file)[0])
            with open(tf_idf_path + '/' + file, 'r', encoding='utf-8') as file:
                lines = file.readlines()
                for i in range(len(lines)):
                    lemma, idf, tf_idf = lines[i].split(' ')
                    tf_idf = tf_idf.strip()
                    zero_matrix[file_number][i] = float(tf_idf)
    return zero_matrix


def vector_search(vector, links, tf_idf):
    distances = dict()
    index = 0
    for page in tf_idf:
        dist = 1 - distance.cosine(vector, page)
        # print(dist)
        if dist > 0 and dist != 1:
            distances[index] = dist
        index += 1
    sorted = sorted(distances.items(), key=lambda item: item[1], reverse=True)

    result = []
    for page_number, score in sorted:
        print("page number- " + str(page_number + 1) + " score= " + str(score))
        if str(page_number + 1) in links:
            result.append(links[str(page_number + 1)])
    return result

def vector_search(vector, links, tf_idf):
    distances = dict()
    index = 0
    for page in tf_idf:
        dist = 1 - distance.cosine(vector, page)
        # print(dist)
        if dist > 0 and dist != 1:
            distances[index] = dist
        index += 1
    sorted_array = sorted(distances.items(), key=lambda item: item[1], reverse=True)

    result = []
    result_index = []
    for page_number, score in sorted_array:
        print("page number- " + str(page_number + 1) + " score= " + str(score))
        if str(page_number + 1) in links:
            result.append(links[str(page_number + 1)])
            result_index.append(page_number + 1)
    return result, result_index

if __name__ == "__main__":
    links = get_links()
    lemmas = get_lemmas()
    tokens = get_tokens()
    tf_idf = get_tf_idf(lemmas)
    while True:
        value = input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å ")
        vector = vectorize(value, lemmas)
        result,indexes = vector_search(vector, links, tf_idf)
        if len(result) == 0:
            print("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")
        else:
            print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
            for i in range(len(result)):
                print("–ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã- " +str(indexes[i]) + " –°—Å—ã–ª–∫–∞ " +str(result[i]))
